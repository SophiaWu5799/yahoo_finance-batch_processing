{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec60da69-9d50-4031-a34a-e0d950c93dc9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#The Pandemicâ€™s Effect on the Stock Market"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27ba1816-c826-4303-a6ab-a82d493b9a4e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###source data in this project: \n",
    "\n",
    "1. sp500 stock list (csv file) in S3 bucket\n",
    "2. covid case global data  (csv file) from mysql \n",
    "3. yahoo finance API: date, open, close, high, low, Adj Close, volume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6b82ba4-edc1-4c7f-8a9b-d2f656386ac8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Planning and preparation:\n",
    "1. upload two csv files from databricks to mysql and S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89eb0a95-d202-4805-8f0c-ae1be4ec32f1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Read the covid case file into a DataFrame\n",
    "\n",
    "# path ='/users/sophiawu/COVID_19_Daily_Counts_of_Cases__Hospitalizations__and_Deaths.csv'\n",
    "\n",
    "# covid_df= spark.read.option('header', True).option('inferSchema', True).csv(path)\n",
    "\n",
    "# #write covid case file to mysql database through jdbc\n",
    "\n",
    "# jdbc_url = 'jdbc:mysql://database.ascendingdc.com:3306/de_001'\n",
    "# user = 'sophiawu'\n",
    "# password = 'welcome'\n",
    "# jdbc_driver = \"com.mysql.jdbc.Driver\"\n",
    "\n",
    "# (covid_df.write \n",
    "#     .format(\"jdbc\") \n",
    "#     .option(\"url\", jdbc_url) \n",
    "#     .option(\"user\", user) \n",
    "#     .option(\"password\", password) \n",
    "#     .option(\"dbtable\", \"covidCase\") \n",
    "#     .mode(\"overwrite\") \n",
    "#     .save()\n",
    "# )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f98dcd7-42f2-4fec-a043-250ba83803c2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# #s3 bucket address \n",
    "# s3_bucket = 'asc-de-training-destination-s3'\n",
    "\n",
    "# dbutils.fs.ls(f\"s3a://{s3_bucket}/users/sophiawu/SP500.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d3a19e6-9410-4f62-922e-1af170c79f84",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.help()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "432d837a-96ac-459a-a5d2-5f42c7ced839",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %pip install yfinance\n",
    "# %pip install bokeh\n",
    "\n",
    "%pip install -r requirements.txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1e477e5-1961-433e-9665-2011c68672ae",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# import the csv file from s3 bucket. \n",
    "# the top 500 largest publicly traded companies in the United States. \n",
    "\n",
    "s3_bucket = 'asc-de-training-destination-s3'\n",
    "path = f\"s3a://{s3_bucket}/users/sophiawu/SP500.csv\"\n",
    "\n",
    "SP500_df = spark.read.format('csv').option('inferSchema', True).option('header', True).load(path)\n",
    "SP500_df.printSchema()\n",
    "\n",
    "#retrieve sticker symbols and convert it to a list\n",
    "my_tickers = [row['Symbol'] for row in SP500_df.select(\"Symbol\").collect() if row['Symbol'] not in ['BRK.B', 'BF.B']]\n",
    "\n",
    "\n",
    "print(type(my_tickers))\n",
    "print(my_tickers)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "434c690d-06d7-474f-aef0-32ca46700adf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#data cleaning: rename sector column \n",
    "\n",
    "SP500_df=SP500_df.withColumnRenamed('GICS?Sector', 'industry')\n",
    "\n",
    "SP500_df.printSchema()\n",
    "display(SP500_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee730cb3-038d-471c-b1b7-2e6107c497b4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "from functools import reduce\n",
    "\n",
    "# # replace with the list of 500 tickers\n",
    "tickers = ['AAPL', 'AMZN', 'GOOG', 'MSFT', 'TSLA', 'META', 'JPM', 'JNJ', 'V', 'PG', 'NVDA', 'HD', 'BAC', 'UNH', 'MA', 'JNJ', 'DIS', 'VZ', 'ADBE','PFE','HLT']  \n",
    "\n",
    "\n",
    "# create a list of DataFrames for each ticker\n",
    "dfs = [spark.createDataFrame(yf.download(\n",
    "                        ticker,\n",
    "                        period=\"10y\",         # time period\n",
    "                        interval=\"1d\",       # trading interval\n",
    "                        ignore_tz=True,      # ignore timezone when aligning data from different exchanges?\n",
    "                        prepost=False).assign(ticker=ticker).reset_index())\n",
    "       for ticker in tickers]\n",
    "\n",
    "print(type(dfs))\n",
    "\n",
    "# union all the DataFrames together\n",
    "df_all = reduce(lambda df1, df2: df1.union(df2), dfs)\n",
    "\n",
    "# show the final DataFrame\n",
    "display(df_all)\n",
    "\n",
    "# print schema\n",
    "df_all.printSchema()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff2d5254-9209-4a01-bf78-03c53011352d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#data preprossing: move column positions, lowercase column names\n",
    "\n",
    "# stock_df = (df_all.select('Date', 'ticker', *[c for c in df_all.columns [1:7]])\n",
    "#             .toDF(*[c.lower() for c in df_all.columns])\n",
    "# )\n",
    "  \n",
    "\n",
    "# stock_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1a9f6b5-629f-4796-b0e1-857484ec7f77",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_all2 = (df_all.withColumnRenamed('Adj close', 'Adj_close')\n",
    "                 .select('Date','ticker','Close','Adj_close','Open', 'High', 'Low','Volume')\n",
    "            \n",
    ")\n",
    "\n",
    "display(df_all2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da507293-325c-45c4-9cf9-05481fe0c722",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# QUESTION 1:\n",
    "# revise timestamp format while keeping it as a timestamp type\n",
    "\n",
    "from pyspark.sql.functions import date_format, to_date\n",
    "\n",
    "df_all3 = (df_all2.withColumn(\"date\", date_format(\"date\", \"yyyy-MM-dd\"))\n",
    "               .withColumn(\"date\", to_date(\"date\", \"yyyy-MM-dd\"))\n",
    "            #    .withColumn('date', to_timestamp(\"date\", \"yyyy-MM-dd\"))\n",
    "\n",
    ")\n",
    "        \n",
    "\n",
    "df_all3.printSchema()\n",
    "display(df_all3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41e08dc2-b7a0-4805-a9f8-dafe2f2be444",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SP500_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b55ca66-f9a1-4df0-af7c-50b4434d6eca",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#join two dataframes to genarate a stock_df which adds an industry column to the df_all\n",
    "\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "stock_df = (df_all3.join(broadcast(SP500_df).select('Symbol','industry'), df_all.ticker == SP500_df.Symbol)\n",
    "                .drop('Symbol')           \n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# stock_df.printSchema()\n",
    "display(stock_df)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44ab2bff-398d-41c9-a61a-3af30b21793c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# if you wanna see a particular stock's performance in a particular year \n",
    "from pyspark.sql.functions import col, year\n",
    "\n",
    "tsla_df = (stock_df.select('*')\n",
    "                   .where((col('ticker')=='TSLA') &(year(col('date')).isin('2020','2021','2022')))\n",
    "                   )\n",
    "display(tsla_df)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab05534f-6099-436f-ac88-339eeb386cad",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#calculate and rank average close price for each stock in each year \n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import *\n",
    "window_spec = Window.partitionBy('year').orderBy(desc_nulls_last('avg_close'))\n",
    "\n",
    "avg_close_df = (stock_df.withColumn('year', year('date'))\n",
    "                            .groupBy('year','ticker','industry')\n",
    "                            .agg(avg('close').alias('avg_close'))\n",
    "                            .withColumn('rank', rank().over(window_spec))\n",
    "                            .orderBy('year')\n",
    "                   )\n",
    "\n",
    "avg_close_df.printSchema()\n",
    "display(avg_close_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc044626-a4e8-4392-ae86-d5f10b2a177e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#calculate market return in each year --adjust close price \n",
    "\n",
    "window_spec = Window.partitionBy(\"ticker\").orderBy(\"year\")\n",
    "\n",
    "\n",
    "\n",
    "market_return_df = (avg_close_df\n",
    "                    .withColumn('prev_year', lag('year').over(window_spec))\n",
    "                    .select('year', 'ticker', (round((((col('avg_close') / lag('avg_close').over(window_spec)) - 1) * 100),2).alias('market_return')), 'industry')\n",
    "                    .orderBy('year')\n",
    "                   )\n",
    "\n",
    "market_return_df.printSchema()\n",
    "display(market_return_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd515289-de5e-4655-bf04-16f2fbb771ec",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# in stock market, the industry benchmark is 8% growth (this is the standard) \n",
    "# if a stock has 20%+ growth, this is hedge fund territory\n",
    "#       >20% : 'Strong performance'\n",
    "#       8% - 20% : 'Positive outlook' \n",
    "#       0-8% :'Mixed performance'\n",
    "#       <0: Underperforming\n",
    "\n",
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "performance_df = (market_return_df\n",
    "                 .select('*')\n",
    "                 .withColumn('performance',\n",
    "                             when(col('market_return') >= '20', 'Strong performance')\n",
    "                             .when(col('market_return').between('8', '20'), 'Positive performance')\n",
    "                             .when(col('market_return').between('0', '8'), 'Mixed performance')\n",
    "                             .when(col('market_return') <=0, 'Underperforming')\n",
    "                             .otherwise('lack of info'))\n",
    "                .filter(col('year') != '2013')\n",
    "                .orderBy('year')\n",
    "                )\n",
    "\n",
    "display(performance_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b36c2cd-40ef-4f2c-98f0-61d6898af9d0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#compare different industries' stock performance in each year\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import avg\n",
    "\n",
    "all_industry_df= (performance_df.select('*')\n",
    "                                    .groupBy('year', 'industry')\n",
    "                                    .agg(avg('market_return').alias('avg_market_return'))\n",
    "                                    .withColumn(' rank', rank().over(Window.partitionBy('year').orderBy(desc('avg_market_return'))))\n",
    "                                               )\n",
    "\n",
    "\n",
    "display(all_industry_df)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df9ddbc4-3564-4c03-a386-8521f95ce72b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#compare different companies' performance in the same industry \n",
    "\n",
    "each_industry_df= (performance_df.select('*')\n",
    "                                    .withColumn(' rank', rank().over(Window.partitionBy('year','industry').orderBy(desc('market_return'))))\n",
    "                                               )\n",
    "\n",
    "display(each_industry_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80d086f7-c576-46d1-907b-66c785c30767",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# List tables in mysql database\n",
    "\n",
    "jdbc_url = 'jdbc:mysql://database.ascendingdc.com:3306/de_001'\n",
    "user = 'sophiawu'\n",
    "password = 'welcome'\n",
    "jdbc_driver = \"com.mysql.jdbc.Driver\"\n",
    "\n",
    "db_name = 'de_001'\n",
    "table_list = (\n",
    "    spark.read.format(\"jdbc\")\n",
    "    .option(\"driver\", jdbc_driver)\n",
    "    .option(\"url\", jdbc_url)\n",
    "    .option(\"dbtable\", \"information_schema.tables\")\n",
    "    .option(\"user\", user)\n",
    "    .option(\"password\", password)\n",
    "    .load()\n",
    "    .filter(f\"table_schema = '{db_name}'\")\n",
    "    .select(\"table_name\")\n",
    ")\n",
    "\n",
    "table_list.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e924c8c2-33ab-45e2-9044-428d6f1e0392",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#read the covid table from mysql to databricks\n",
    "\n",
    "covid_case_df = (\n",
    "    spark.read.format(\"jdbc\")\n",
    "    .option(\"driver\", jdbc_driver)\n",
    "    .option(\"url\", jdbc_url)\n",
    "    .option(\"dbtable\", \"covidCase\")\n",
    "    .option(\"user\", user)\n",
    "    .option(\"password\", password)\n",
    "    .load()\n",
    ")\n",
    "\n",
    "display(covid_case_df)\n",
    "covid_case_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5603c721-d89c-4e7a-bf55-645e1af83359",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#change date format from 3/20/20 to 2023-03-20\n",
    "\n",
    "\n",
    "covid_case_df = (covid_case_df.withColumn('date_of_interest', date_format('date_of_interest','yyyy-MM-dd'))\n",
    "                             .withColumn('date_of_interest', to_date('date_of_interest','yyyy-MM-dd'))\n",
    ")\n",
    "\n",
    "                   \n",
    "display(covid_case_df)\n",
    "covid_case_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d711bf2-d381-43b3-b6aa-df0caa541180",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#correlation between COVID-19 case numbers and stock price in 2020\n",
    "\n",
    "# A correlation coefficient of 1 indicates a perfect positive correlation, \n",
    "# while a value of -1 indicates a perfect negative correlation, \n",
    "# and a value of 0 indicates no correlation.\n",
    "# In general, correlation coefficients above 0.7 or below -0.7 are often considered strong correlations.\n",
    "\n",
    "\n",
    "# Quarterly 2 report\n",
    "from pyspark.sql.functions import col, corr\n",
    "\n",
    "# Join the two dataframes on the date column\n",
    "quarter2_df =(stock_df.select('date', 'ticker', 'Close', 'industry')\n",
    "                        .where(col('date').between('2020-03-01','2020-05-31'))\n",
    "                        .join(broadcast(covid_case_df).select('date_of_interest', 'CASE_COUNT'),col('date')==col('date_of_interest'))\n",
    "                        .drop('date_of_interest')\n",
    "                        )\n",
    "\n",
    "from pyspark.sql.functions import corr\n",
    "\n",
    "# Compute the correlation between close price and case number\n",
    "corr_q2_df = (quarter2_df.groupBy(\"ticker\")\n",
    "                        .agg(corr('Close', 'CASE_COUNT').alias('correlation'))\n",
    "                        .withColumn('window', lit('Q2 2020'))\n",
    ")\n",
    "\n",
    "display(corr_q2_df)\n",
    "\n",
    "#join the corr_df with sp500_df to caculate the impact of covid cases on different industry \n",
    "\n",
    "industry_q2_df = (corr_q2_df\n",
    "                            .join(SP500_df, corr_q2_df.ticker == SP500_df.Symbol)\n",
    "                            .groupBy('window','industry')\n",
    "                            .agg(avg(col('correlation')).alias('avg_corr'))\n",
    "                            .orderBy('avg_corr')\n",
    "           \n",
    "                   )\n",
    " \n",
    "display(industry_q2_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f3ae661-7ff1-49d4-ac7b-f4d1c56f120a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#quarter 3 report\n",
    "\n",
    "quarter3_df =(stock_df.select('date', 'ticker', 'Close', 'industry')\n",
    "                        .where(col('date').between('2020-06-01','2020-08-31'))\n",
    "                        .join(broadcast(covid_case_df).select('date_of_interest', 'CASE_COUNT'),col('date')==col('date_of_interest'))\n",
    "                        .drop('date_of_interest')\n",
    "                        )\n",
    "\n",
    "# Compute the correlation between close price and covid case number\n",
    "corr_q3_df = (quarter3_df.groupBy(\"ticker\")\n",
    "                        .agg(corr('Close', 'CASE_COUNT').alias('correlation'))\n",
    "                        .withColumn('window', lit('Q3 2020'))\n",
    ")\n",
    "\n",
    "display(corr_q3_df)\n",
    "\n",
    "industry_q3_df = (corr_q3_df\n",
    "                            .join(SP500_df, corr_q3_df.ticker == SP500_df.Symbol)\n",
    "                            .groupBy('window','industry')\n",
    "                            .agg(avg(col('correlation')).alias('avg_corr'))\n",
    "                            .orderBy('avg_corr')\n",
    "           \n",
    "                   )\n",
    " \n",
    "display(industry_q3_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "275abe26-bacd-4a2c-8d53-eac712d6db5b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#quarter 4 report\n",
    "\n",
    "quarter4_df =(stock_df.select('date', 'ticker', 'Close', 'industry')\n",
    "                        .where(col('date').between('2020-10-01','2020-12-31'))\n",
    "                        .join(broadcast(covid_case_df).select('date_of_interest', 'CASE_COUNT'),col('date')==col('date_of_interest'))\n",
    "                        .drop('date_of_interest')\n",
    "                        )\n",
    "\n",
    "# Compute the correlation between close price and covid case number\n",
    "corr_q4_df = (quarter4_df.groupBy(\"ticker\")\n",
    "                        .agg(corr('Close', 'CASE_COUNT').alias('correlation'))\n",
    "                        .withColumn('window', lit('Q4 2020'))\n",
    ")\n",
    "\n",
    "display(corr_q4_df)\n",
    "\n",
    "\n",
    "industry_q4_df = (corr_q4_df\n",
    "                            .join(SP500_df, corr_q4_df.ticker == SP500_df.Symbol)\n",
    "                            .groupBy('window','industry')\n",
    "                            .agg(avg(col('correlation')).alias('avg_corr'))\n",
    "                            .orderBy('avg_corr')\n",
    "           \n",
    "                   )\n",
    " \n",
    "display(industry_q4_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a8cc47bf-099d-4aa1-9c1c-e2a41e6c6b46",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#calculate market return in each quarter in 2020 based on close price \n",
    "\n",
    "\n",
    "#step1 : calculate quarterly average in 2020\n",
    "\n",
    "quarterly_avg_df = (stock_df.withColumn('quarter', quarter('date'))\n",
    "                            .groupBy('quarter','ticker','industry')\n",
    "                            .agg(avg('close').alias('quarterly_avg_close'))\n",
    "                            .withColumn('rank', rank().over(window_spec))\n",
    "                            .orderBy('quarter')\n",
    "                   )\n",
    "\n",
    "# quarterly_avg_df.printSchema()\n",
    "# display(quarterly_avg_df)\n",
    "\n",
    "# window_spec = Window.partitionBy(\"ticker\").orderBy(\"quarter\")\n",
    "\n",
    "\n",
    "# market_return_df = (avg_close_df\n",
    "#                     .withColumn('prev_year', lag('year').over(window_spec))\n",
    "#                     .select('year', 'ticker', (round((((col('avg_close') / lag('avg_close').over(window_spec)) - 1) * 100),2).alias('market_return')), 'industry')\n",
    "#                     .orderBy('year')\n",
    "#                    )\n",
    "\n",
    "# market_return_df.printSchema()\n",
    "# display(market_return_df)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2215212627979994,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "yahoo_finance-batch_processing",
   "notebookOrigID": 2215212627979952,
   "widgets": {
    "a": {
     "currentValue": "tom",
     "nuid": "0ce4a35c-08fb-445b-a70f-d88bb0ce1c32",
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "null",
      "label": "name",
      "name": "a",
      "options": {
       "widgetType": "text",
       "validationRegex": null
      }
     }
    },
    "s3_bucket": {
     "currentValue": "",
     "nuid": "59a6ac0f-efde-4aed-bc2e-ebbc47c0df2b",
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "The S3 buket name for storing data:",
      "name": "s3_bucket",
      "options": {
       "widgetType": "text",
       "validationRegex": null
      }
     }
    },
    "ticker": {
     "currentValue": "AAPL, AMZN",
     "nuid": "05cbbe6d-dae1-49c3-ad79-ea7c2635a81e",
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "Please input the tickers you need:",
      "name": "ticker",
      "options": {
       "widgetType": "text",
       "validationRegex": null
      }
     }
    }
   }
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
